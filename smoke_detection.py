# -*- coding: utf-8 -*-
"""SMOKE_DETECTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1yo2NAvHhS3FV1J7nPQ7u8g8jbEwpsIAN
"""

from google.colab import files
uploaded = files.upload()

# Libraries
import numpy as np 
import pandas as pd
import time
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.graph_objs as go
import plotly.offline as py

from sklearn.ensemble import GradientBoostingClassifier,AdaBoostClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import ExtraTreeClassifier, DecisionTreeClassifier
# ANN

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, f1_score

import seaborn as sns
import matplotlib.pyplot as plt

# create a custom color palette with three colors
my_palette = sns.color_palette(['#377eb8', '#ff7f00', '#4daf4a'])

# Reading the dataset
dataset = pd.read_csv('/content/smoke_detection.csv',index_col = False)
data = dataset.copy()

"""#Data accessing"""

# Displaying 5 random samples from the dataset first 5 data
data.sample(5)

data.head()

data.tail()

data.index

data.shape

data.columns

data.info()

data.isna().sum()

data = data.drop(['column_a'], axis =1)

for col in data.columns:
    print('{} : {}'.format(col, data[col].unique()))

data.mean()

data.duplicated().sum()

data[20:30]

data.describe().T

"""#Visualization starts from here"""

data.count().plot.bar()

sns.heatmap(data.isnull(), cmap='viridis')

plt.figure(figsize=(10, 10))
sns.heatmap(data.corr(), cbar=True, annot=True, cmap='Blues')

data.count().plot.barh()

data.boxplot(column=['pm2_5', 'cnt'])
plt.show()

# Set figure size
plt.figure(figsize=(10, 6))

# Plot temperature, humidity, and TVOC
plt.plot(data['utc'], data['temperature_c'], label='Temperature')
plt.plot(data['utc'], data['humidity'], label='Humidity')
plt.plot(data['utc'], data['tvoc_ppb'], label='TVOC')

# Set axis labels and title
plt.xlabel('utc')
plt.ylabel('Reading')
plt.title('Temperature, Humidity, and TVOC Readings over Time')

# Add legend and show the plot
plt.legend()
plt.show()

# Set figure size
plt.figure(figsize=(10, 6))

# Plot temperature, humidity, and pressure_hpa
plt.plot(data['utc'], data['temperature_c'], label='Temperature')
plt.plot(data['utc'], data['humidity'], label='Humidity')
plt.plot(data['utc'], data['pressure_hpa'], label='Pressure')

# Set axis labels and title
plt.xlabel('utc')
plt.ylabel('Reading')
plt.title('Temperature, Humidity, and pressure Readings over Time')

# Add legend and show the plot
plt.legend()
plt.show()



# Set figure size
plt.figure(figsize=(10, 6))

# Plot temperature, humidity, and pressure_hpa
plt.plot(data['temperature_c'], data['fire_alarm'], label='temp')
plt.plot(data['humidity'], data['fire_alarm'], label='humidity')
#plt.plot(data['pressure_hpa'], data['fire_alarm'], label='pressure')

# Set axis labels and title
plt.xlabel('reading')
plt.ylabel('fire')
plt.title('Temperature, Humidity, and pressure Readings over Time')

# Add legend and show the plot
plt.legend()
plt.show()

data.plot.area(x='temperature_c',y='fire_alarm')

data.plot.area(x='humidity',y='fire_alarm')

data.plot.area(x='pressure_hpa',y='fire_alarm')

sns.countplot(data=data)

# create a new column for positive values
data['temp_pos'] = data['temperature_c'].apply(lambda x: max(x, 0))

# create a new column for negative values
data['temp_neg'] = data['temperature_c'].apply(lambda x: min(x, 0))

# create a stacked area plot with the new columns
data.plot.area(y=['temp_pos', 'temp_neg'], stacked=True)

sns.boxplot(data=data.temperature_c)

data.temperature_c.value_counts().plot.bar()

data.humidity.value_counts().plot.bar()

data.cnt.value_counts().plot.bar()

data.fire_alarm.value_counts().plot.bar()

data.tvoc_ppb.value_counts().plot.bar()

data.eco2_ppm.value_counts().plot.bar()

data.head()

# Set figure size
plt.figure(figsize=(20,20))

data.plot.box()

data.plot.scatter(x='temperature_c', y='fire_alarm')

data.plot.scatter(y='temperature_c', x='humidity')

data.plot.scatter(x='temperature_c', y='cnt')

data.plot.scatter(x='tvoc_ppb', y='cnt')

data.plot.scatter(x='temperature_c', y='tvoc_ppb')

data.plot.scatter(y='humidity', x='tvoc_ppb')

plt.figure(figsize=(20,20))
data.hist()

sns.relplot(x='temperature_c', y='humidity', hue='fire_alarm', data=data, height=6)

data.head()

sns.relplot(x='cnt', y='tvoc_ppb', hue='fire_alarm', data=data, height=6)

sns.relplot(x='raw_h2', y='eco2_ppm', hue='fire_alarm', data=data, height=5,aspect=2)

"""calculations

#MAIN visualisation
"""

data.columns

continuous_vars = ['utc', 'temperature_c', 'humidity', 'tvoc_ppb', 'eco2_ppm', 'raw_h2',
       'raw_ethanol', 'pressure_hpa', 'pm1_0', 'pm2_5', 'nc0_5', 'nc1_0',
       'nc2_5', 'cnt']

for var in continuous_vars:
    median = data[var].median()
    std = data[var].std()
    print(f'{var} median: {median:.2f}')
    print(f'{var} std: {std:.2f}')

"""Here's a possible mathematical procedure to calculate the presence of fire or smoke from the given data:

1. Calculate the median and standard deviation for each continuous variable (Temperature[C], Humidity[%], TVOC[ppb], eCO2[ppm], Raw H2, Raw Ethanol, Pressure[hPa], PM1.0, PM2.5, NC0.5, NC1.0, NC2.5, CNT) across all observations. These values will be used as reference points for detecting anomalies.

2. For each observation, check if the Fire Alarm variable is set to 1. If it is, then there is a high likelihood that a fire has been detected. In this case, emergency procedures should be followed immediately.

3. For each observation, check if the temperature reading is greater than the median temperature plus 3 standard deviations. If it is, then it could indicate the presence of a fire.

4. For each observation, check if the Raw H2 and Raw Ethanol readings are greater than the median plus 3 standard deviations. If they are, then it could indicate the presence of smoke.

5. For each observation, check if the TVOC and eCO2 readings are greater than the median plus 3 standard deviations. If they are, then it could indicate the presence of smoke.

6. For each observation, check if the PM1.0, PM2.5, NC0.5, NC1.0, NC2.5, and CNT readings are greater than the median plus 3 standard deviations. If they are, then it could indicate the presence of smoke particles in the air.

7. If any of the above checks are positive, then there is a potential presence of fire or smoke. Further analysis and confirmation should be done before taking any action.

It's important to note that the specific threshold values used (such as the 3 standard deviation cutoff) will depend on the specific context and requirements of the application, and should be validated and tuned accordingly.
"""

continuous_vars = ['utc', 'temperature_c', 'humidity', 'tvoc_ppb', 'eco2_ppm', 'raw_h2',
       'raw_ethanol', 'pressure_hpa', 'pm1_0', 'pm2_5', 'nc0_5', 'nc1_0',
       'nc2_5', 'cnt']

for index, row in data.iterrows():
    temp = row['temperature_c']
    median_temp = data['temperature_c'].median()
    std_temp = data['temperature_c'].std()
    if temp > median_temp + 3*std_temp:
        print(f'Possible fire detected in observation {index}. Temperature: {temp:.2f}Â°C')

fig, ax = plt.subplots()
ax.plot(data['utc'], data['temperature_c'], label='Temperature')
for index, row in data.iterrows():
    temp = row['temperature_c']
    median_temp = data['temperature_c'].median()
    std_temp = data['temperature_c'].std()
    if temp > median_temp + 3*std_temp:
        ax.scatter(row['utc'], temp, color='red', marker='*', label='Possible Fire')
ax.legend()
plt.xlabel('utc')
plt.ylabel('temperature_c')
plt.title('Temperature Readings')
plt.show()

data.columns

"""fig, ax = plt.subplots()
ax.scatter(data['raw_h2'], data['raw_ethanol'], label='Raw H2 and Ethanol')

for index, row in data.iterrows():
    raw_h2 = row['raw_h2']
    raw_ethanol = row['raw_ethanol']
    median_h2 = data['raw_h2'].median()
    std_h2 = data['raw_h2'].std()
    median_ethanol = data['raw_ethanol'].median()
    std_ethanol = data['raw_ethanol'].std()
    if raw_h2 > median_h2 + 3*std_h2 or raw_ethanol > median_ethanol + 3*std_ethanol:
       ax.scatter(raw_h2, raw_ethanol, color='red', marker='*', label='Possible Smoke')
ax.legend()
plt.xlabel('raw_h2')
plt.ylabel('raw_ethanol')
plt.title('Raw H2 and Ethanol Readings')
plt.show()
"""



"""fig, ax = plt.subplots()
ax.scatter(df['Raw H2'], df['Raw Ethanol'], label='Raw H2 and Ethanol')
possible_smoke = []
for index, row in df.iterrows():
    raw_h2 = row['Raw H2']
    raw_ethanol = row['Raw Ethanol']
    median_h2 = df['Raw H2'].median()
    std_h2 = df['Raw H2'].std()
    median_ethanol = df['Raw Ethanol'].median()
    std_ethanol = df['Raw Ethanol'].std()
    if raw_h2 > median_h2 + 3*std_h2 or raw_ethanol > median_ethanol + 3*std_ethanol:
        possible_smoke.append(ax.scatter(raw_h2, raw_ethanol, color='red', marker='*'))
if possible_smoke:
    ax.legend([possible_smoke[0]], ['Possible Smoke'])
plt.xlabel('Raw H2')
plt.ylabel('Raw Ethanol')
plt.title('Raw H2 and Ethanol Readings')
plt.show()
"""

fig, ax = plt.subplots()
ax.scatter(data['raw_h2'], data['raw_ethanol'], label='Raw H2 and Ethanol')
possible_smoke = []
for index, row in data.iterrows():
    raw_h2 = row['raw_h2']
    raw_ethanol = row['raw_ethanol']
    median_h2 = data['raw_h2'].median()
    std_h2 = data['raw_h2'].std()
    median_ethanol = data['raw_ethanol'].median()
    std_ethanol = data['raw_ethanol'].std()
    if raw_h2 > median_h2 + 3*std_h2 or raw_ethanol > median_ethanol + 3*std_ethanol:
        possible_smoke.append(ax.scatter(raw_h2, raw_ethanol, color='red', marker='*'))
if possible_smoke:
    ax.legend([possible_smoke[0]], ['Possible Smoke'])
plt.xlabel('raw_h2')
plt.ylabel('raw_ethanol')
plt.title('Raw H2 and Ethanol Readings')
plt.show()

# Calculate median and standard deviation for TVOC and eCO2
tvoc_median = data['tvoc_ppb'].median()
tvoc_std = data['tvoc_ppb'].std()
eco2_median = data['eco2_ppm'].median()
eco2_std = data['eco2_ppm'].std()

# Create a new column indicating the possible presence of smoke
data['Smoke'] = ((data['tvoc_ppb'] > (tvoc_median + 3*tvoc_std)) | 
                 (data['eco2_ppm'] > (eco2_median + 3*eco2_std))).astype(int)

# Visualize the data
plt.plot(data['utc'], data['tvoc_ppb'], label='TVOC')
plt.plot(data['utc'], data['eco2_ppm'], label='eCO2')
plt.scatter(data[data['Smoke'] == 1]['utc'], 
            data[data['Smoke'] == 1]['tvoc_ppb'], 
            c='r', marker='x', label='Possible smoke')
plt.scatter(data[data['Smoke'] == 1]['utc'], 
            data[data['Smoke'] == 1]['eco2_ppm'], 
            c='r', marker='x')
plt.legend()
plt.xlabel('UTC')
plt.ylabel('Concentration')
plt.title('TVOC and eCO2 readings with possible smoke detection')
plt.show()

# Calculate median and standard deviation for each continuous variable
median = data.median()
std = data.std()

# Check for presence of smoke particles
smoke_mask = ((data['pm1_0'] > median['pm1_0'] + 3*std['pm1_0']) |
              (data['pm2_5'] > median['pm2_5'] + 3*std['pm2_5']) |
              (data['nc0_5'] > median['nc0_5'] + 3*std['nc0_5']) |
              (data['nc1_0'] > median['nc1_0'] + 3*std['nc1_0']) |
              (data['nc2_5'] > median['nc2_5'] + 3*std['nc2_5']) |
              (data['cnt'] > median['cnt'] + 3*std['cnt']))

# Plot smoke particles
plt.plot(data['utc'], data['pm1_0'], label='pm1_0')
plt.plot(data['utc'], data['pm2_5'], label='pm2_5')
plt.plot(data['utc'], data['nc0_5'], label='nc0_5')
plt.plot(data['utc'], data['nc1_0'], label='nc1_0')
plt.plot(data['utc'], data['nc2_5'], label='nc2_5')
plt.plot(data['utc'], data['cnt'], label='cnt')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'pm1_0'], color='red', label='Possible Smoke')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'pm2_5'], color='red')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'nc0_5'], color='red')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'nc1_0'], color='red')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'nc2_5'], color='red')
plt.scatter(data.loc[smoke_mask, 'utc'], data.loc[smoke_mask, 'cnt'], color='red')
plt.xlabel('utc')
plt.ylabel('Particle Count')
plt.legend()
plt.show()

plt.figure(figsize=(12,8))
data_4 = data.corr()["fire_alarm"].sort_values(ascending=False)
indices = data_4.index
labels = []
corr = []
for i in range(1, len(indices)):
    labels.append(indices[i])
    corr.append(data_4[i])
sns.barplot(x=corr, y=labels, palette='mako')
plt.title('Correlation coefficient between different features and Fire Alarm ')
plt.show()

plt.figure(figsize = (15,10))
corr = data.corr(method='spearman')
mask = np.triu(np.ones_like(corr, dtype=bool))
cormat = sns.heatmap(corr, mask=mask, annot=True, cmap='YlGnBu', linewidths=1, fmt=".2f")
cormat.set_title('Correlation Matrix')
plt.show()

yes_fire = data[(data['fire_alarm'] != 0)]
no_fire = data[(data['fire_alarm'] == 0)]

trace = go.Bar(x = (len(no_fire), len(yes_fire)), y = ['No_fire', 'Yes_fire'], orientation = 'h', opacity = 0.8, marker=dict(
        color=['orange', 'darkblue'],
        line=dict(color='#000000',width=1.5)))

layout = dict(title =  'Count of Fire Alarm variable')
                    
fig = dict(data = [trace], layout=layout)
py.iplot(fig)

trace = go.Pie(labels = ['Yes_Fire', 'No_Fire'], values = data['fire_alarm'].value_counts(), 
               textfont=dict(size=15), opacity = 0.8,
               marker=dict(colors=['ldarkblue','orange'], 
                           line=dict(color='#000000', width=1.5)))


layout = dict(title =  'Distribution of Fire Alarm variable')
           
fig = dict(data = [trace], layout=layout)
py.iplot(fig)

plt.figure(figsize=(12,9))
sns.distplot(data['humidity'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['temperature_c'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['tvoc_ppb'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['eco2_ppm'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['raw_ethanol'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['raw_h2'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['pressure_hpa'])
plt.show()

plt.figure(figsize=(12,9))
sns.distplot(data['cnt'])
plt.show()

"""#Training of the model"""

input_cols = list(data.columns)[1:-2]
target_col = ['fire_alarm']

train_inputs = data[input_cols].copy()
train_targets = data[target_col].copy()

X_train, X_test, y_train, y_test = train_test_split(train_inputs, train_targets, test_size=0.2,random_state=42)

X_train.shape

X_test.shape

X_train.head()

scaler = MinMaxScaler().fit(data[input_cols])

X_train[input_cols] = scaler.transform(X_train[input_cols])
X_test[input_cols] = scaler.transform(X_test[input_cols])

X_test.describe().loc[['min', 'max']]

y_train

# This function evaluates the train dataset
def train_acc_model(models):
    model_name_list = []
    confusion_metrx_list = []
    execution_time_list = []
    model_score_list = []
    auc_list = []
    f1_score_list = []
    accuracy_score_list = []
    #print('Training dataset Evaluation with different metrix')
    
    for model in models:
        
        start = time.time()
        model.fit(X_train, y_train.values.ravel())
        end = time.time()
        
        model_name_list.append(type(model).__name__)

        #model prediction
        train_preds = model.predict(X_train)

        # model_score
        ms_train_preds = model.score(X_train, y_train)
        model_score_list.append(ms_train_preds)

        # Accuracy_score metrix
        as_train_acc = accuracy_score(y_train, train_preds)
        accuracy_score_list.append(as_train_acc)

        # confusion metrix
        cf_train_acc = confusion_matrix(y_train, train_preds)
        confusion_metrx_list.append(cf_train_acc)

        #Area under curve
        auc_train_acc = roc_auc_score(y_train, train_preds)
        auc_list.append(auc_train_acc)

        # f1 score
        f1_train_acc = f1_score(y_train, train_preds)
        f1_score_list.append(f1_train_acc)
        
        execution_time_list.append(end-start)

        Dict = {'Model':model_name_list,
                'Execution Time(sec)':execution_time_list,
                'Model Score':model_score_list, 
                'Confusion Metrx':confusion_metrx_list,
                'AUC':auc_list,
                'F1 Score ':f1_score_list, 
                'Accuracy Score':accuracy_score_list}
    return pd.DataFrame(Dict)

# This function evaluates the test dataset
def test_acc_model(models):
    model_name_list = []
    confusion_metrx_list = []
    execution_time_list = []
    model_score_list = []
    auc_list = []
    f1_score_list = []
    accuracy_score_list = []
    feature_importance = []
    feature_imp_name = []
    
    #print('Evaluating Validation dataset with different metrix')
    
    for model in models:
        start = time.time()
        model.fit(X_train, y_train.values.ravel())
        end = time.time()
        try:
            feature_importance.append(pd.DataFrame({
                (str(model) + ': Features'): X_train.columns,
                'Importance': model.feature_importances_
            }).sort_values('Importance', ascending=False))
        except:
            pass
        
        model_name_list.append(type(model).__name__)

        #model prediction
        test_preds = model.predict(X_test)

        # model_score
        ms_val_preds = model.score(X_test, y_test)
        model_score_list.append(ms_val_preds)

        # Accuracy_score metrix
        as_test_acc = accuracy_score(y_test, test_preds)
        accuracy_score_list.append(as_test_acc)

        # confusion metrix
        cf_test_acc = confusion_matrix(y_test, test_preds)
        confusion_metrx_list.append(cf_test_acc)

        #Area under curve
        auc_test_acc = roc_auc_score(y_test, test_preds)
        auc_list.append(auc_test_acc)

        # f1 score
        f1_test_acc = f1_score(y_test, test_preds)
        f1_score_list.append(f1_test_acc)
        
        execution_time_list.append(end-start)

    Dict = {'Model':model_name_list,
            'Execution Time(sec)':execution_time_list,
            'Model Score':model_score_list, 
            'Confusion Metrx':confusion_metrx_list,
            'AUC':auc_list,
            'F1 Score ':f1_score_list, 
            'Accuracy Score':accuracy_score_list}

    #feat_imp_dict = {'Model name':feature_imp_name,
     #                'Feature Importance':feature_importance}
    
    return pd.DataFrame(Dict), feature_importance

# List of different machine learning models
models = [
    LogisticRegression(solver='lbfgs', max_iter=1000), 
    GradientBoostingClassifier(), 
    AdaBoostClassifier(), 
    ExtraTreeClassifier(), 
    DecisionTreeClassifier()
]

# The training accuracy
train_acc_model(models)

testing_acc, feature_importance = test_acc_model(models)

testing_acc

# initializing to index 0 to plot feature for first model
for each_model in range(len(feature_importance)):
    plt.title('Feature Importance')
    sns.barplot(data=feature_importance[each_model], x='Importance', y = feature_importance[each_model].columns[0]);
    plt.show()

"""Conclusion 
So far we have been able to perform EDA on the Smoke Dataset. There was no missing so no much of cleaning done. we droped 3 columns that are not neccessary for prediction ('column_a', 'utc' and 'cnt'). We then splited the dataset into train and test and then trained five models to make make prediction for us and finally we platted a graph for the five model to see the feature importance for each.

#Best from the rest algo
"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.dummy import DummyClassifier
from sklearn.tree import ExtraTreeClassifier 
from sklearn.metrics import accuracy_score

models = [KNeighborsClassifier(),LogisticRegression(),RandomForestClassifier(),GradientBoostingClassifier(),
         AdaBoostClassifier(),SVC(),GaussianNB(),DummyClassifier(),ExtraTreeClassifier()]

X = data.copy()
X.drop('fire_alarm',axis = 1,inplace = True)
y = data['fire_alarm']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=0)

std = StandardScaler()
X_train = std.fit_transform(X_train)
X_test = std.transform(X_test)

Name = []
Accuracy = []
Time_Taken = []
for model in models:
    Name.append(type(model).__name__)
    begin = time.time()
    model.fit(X_train,y_train)
    prediction = model.predict(X_test)
    end = time.time()
    accuracyScore = accuracy_score(prediction,y_test)
    Accuracy.append(accuracyScore)
    Time_Taken.append(end-begin)

Dict = {'Name':Name,'Accuracy':Accuracy,'Time Taken':Time_Taken}
model_df = pd.DataFrame(Dict)
model_df

import matplotlib.pyplot as plt
import pandas as pd

# Define the data as a pandas dataframe
data = {'Name': ['KNeighborsClassifier', 'LogisticRegression', 'RandomForestClassifier',
                 'GradientBoostingClassifier', 'AdaBoostClassifier', 'SVC', 'GaussianNB',
                 'DummyClassifier', 'ExtraTreeClassifier'],
        'Accuracy': [0.999441, 0.989222, 1.000000, 1.000000, 0.999920, 0.999281, 0.825962,
                     0.712199, 0.999681],
        'Time Taken': [1.438838, 0.493939, 4.987484, 17.464838, 3.969565, 6.611750, 0.016760,
                       0.002132, 0.020339]}

df = pd.DataFrame(data)

# Sort the dataframe by accuracy in descending order
df = df.sort_values(by=['Accuracy'], ascending=False)

# Create a bar chart for the accuracy of each model
fig, ax = plt.subplots(figsize=(12, 6))
ax.bar(df['Name'], df['Accuracy'])
ax.set_title('Accuracy of Classification Models')
ax.set_xlabel('Model')
ax.set_ylabel('Accuracy')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

# Create a bar chart for the time taken by each model
fig, ax = plt.subplots(figsize=(12, 6))
ax.bar(df['Name'], df['Time Taken'])
ax.set_title('Time Taken by Classification Models')
ax.set_xlabel('Model')
ax.set_ylabel('Time Taken (seconds)')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression

# Generate some random data
np.random.seed(123)
X = np.random.randn(100, 2)
y = (X[:, 0] + X[:, 1] > 0).astype(int)

# Create the logistic regression model
logreg = LogisticRegression(solver='lbfgs', max_iter=1000)

# Fit the model to the data
logreg.fit(X, y)

# Create a meshgrid to plot the decision boundary
x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5
y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

# Predict the class of each point on the meshgrid
Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

# Plot the decision boundary
plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Logistic Regression Decision Boundary')
plt.show()

# Create the decision tree model
dt_model = DecisionTreeClassifier()

# Fit the model to the data
dt_model.fit(X_train, y_train)

# Visualize the decision tree
fig, ax = plt.subplots(figsize=(20, 10))
plot_tree(dt_model, ax=ax, feature_names=input_cols, fontsize=7, filled=True, class_names=['Class 0', 'Class 1'])
plt.show()

# Create the decision tree model
dt_model =ExtraTreeClassifier()

# Fit the model to the data
dt_model.fit(X_train, y_train)

# Visualize the decision tree with larger font size and two colors
fig, ax = plt.subplots(figsize=(35, 30))
plot_tree(dt_model, ax=ax, feature_names=input_cols, fontsize=6.5,filled=True, class_names=['Class 0', 'Class 1'])
plt.show()

# Create a gradient boosting classifier
gb_model = GradientBoostingClassifier()

# Fit the model to the data
gb_model.fit(X_train, y_train)

# Extract a decision tree from the gradient boosting classifier
dt_model = gb_model.estimators_[0, 0]

# Visualize the decision tree with adjusted plot size and font size
fig, ax = plt.subplots(figsize=(12, 5))
plot_tree(dt_model, ax=ax, feature_names=input_cols, fontsize=6.5,filled=True, class_names=['Class 0', 'Class 1'])
plt.show()

